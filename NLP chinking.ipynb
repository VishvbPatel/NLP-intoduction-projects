{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the chunkiung of the tagged words has been done. The chunking is the process of grouping of the words on the basis of their tags in terms of nouns or verbs and chinking is the process of removal of something that is not required. So, first the libraries has been added and then  the train text and sample text has been added. First of all the tagging of the words has been done with the help of \"PunktSentenceTokenizer\" and \"pos_tag\". After successfully tagging, the chunking has been done with the help of \"RegexpParser\". And chinking is not a seperate process, it's just the not required words of phrases are defined that will not be included in the chunk And then printing the chunked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                        #importing nltk library\n",
    "from nltk.corpus import state_union                #importing state_union for adding the speech\n",
    "from nltk.tokenize import PunktSentenceTokenizer   #importing the \"PunktSentenceTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")  #adding train text\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\") #adding sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)  #tokenizing train text with the help of \"PunktSentenceTokenizer\"\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)     #tokenizing the sample text with the help of trained train_text\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)    #seperating words with word_tokenize\n",
    "            tagged = nltk.pos_tag(words)     #seperating tags with pos_tag\n",
    "            chunkGram = r\"\"\"chunk: {<.*>+}\n",
    "                                          }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)  \n",
    "            chunked = chunkParser.parse(tagged)      #chunking the tagged words\n",
    "            \n",
    "            \n",
    "            chunked.draw()                    #printing chunked words\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "process_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
